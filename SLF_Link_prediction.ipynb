{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLF_Link_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zgwd7JthTt8",
        "colab_type": "text"
      },
      "source": [
        "# Reading a data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4FeLxKGoUiM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "be5fd4e1-8d96-40c4-e347-cd3fbd0d3a56"
      },
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random as rd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "data_path = '/content/soc-sign-epinions.txt'\n",
        "dim = 32\n",
        "learning_rate_0 = 0.025\n",
        "noise_size = 5\n",
        "max_iter = 3\n",
        "p0 = 0.001\n",
        "\n",
        "# Logistic regression function\n",
        "def fa(x):\n",
        "    if x > 15:\n",
        "        tmp = 1\n",
        "    else:\n",
        "        tmp = p0 * np.exp(x) / (1 + p0 * (np.exp(x) - 1))\n",
        "    return tmp\n",
        "\n",
        "\n",
        "G = nx.DiGraph()\n",
        "with open(data_path) as f:\n",
        "    for line in f:\n",
        "        line = line.strip().split()\n",
        "        #print(int(line[2]))\n",
        "        if int(line[2]) != 0:\n",
        "            G.add_edge(int(line[0]), int(line[1]), weight=int(line[2]))\n",
        "    \n",
        "max_node_id = max(G.nodes())\n",
        "undi_G = G.to_undirected()\n",
        "nodes = list(G.nodes())\n",
        "edges = np.array(G.edges())\n",
        "\n",
        "\n",
        "edgeL=[]\n",
        "with open(data_path) as f:\n",
        "  ss_links=f.read().splitlines()\n",
        "\n",
        "len(ss_links)\n",
        "\n",
        "edgeL1=[]\n",
        "edgeL2=[]\n",
        "signVal=[]\n",
        "for i in tqdm(ss_links):\n",
        "  edgeL1.append(int(i.split()[0]))\n",
        "  edgeL2.append(int(i.split()[1]))\n",
        "  signVal.append(int(i.split()[2]))\n",
        "\n",
        "ss_df=pd.DataFrame({\"source\": edgeL1, \"destination\": edgeL2, \"sign\":signVal})\n",
        "\n",
        "ss_df.head()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 841372/841372 [00:01<00:00, 575638.08it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>destination</th>\n",
              "      <th>sign</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>128552</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>155</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   source  destination  sign\n",
              "0       0            1    -1\n",
              "1       1       128552    -1\n",
              "2       2            3     1\n",
              "3       4            5    -1\n",
              "4       4          155    -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRxAL_v3tPQP",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Visualization\n",
        "\n",
        "To understand Dataset nature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azD0sP415TZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "7a875b99-3b01-4265-d571-d0dc1d20362b"
      },
      "source": [
        "import collections\n",
        "c = collections.Counter(signVal)\n",
        "c = sorted(c.items())\n",
        "\n",
        "sign_num=[i[0] for i in c]\n",
        "sign_freq=[i[1] for i in c]\n",
        "\n",
        "print(sign_freq)\n",
        "\n",
        "f, ax = plt.subplots()\n",
        "\n",
        "plt.bar(sign_num, sign_freq)\n",
        "plt.title(\"Signed links distribution\", fontweight='bold')\n",
        "plt.xlabel(\"Sign values\")\n",
        "plt.ylabel(\"No. of links\")\n",
        "ax.set_xticks(range(-1, 2))\n",
        "\n",
        "for x,y in zip(sign_num,sign_freq):\n",
        "    label = \"{:}\".format(y)\n",
        "    plt.annotate(label, # this is the text\n",
        "                 (x,y), # this is the point to label\n",
        "                 textcoords=\"offset points\", # how to position the text\n",
        "                 xytext=(0,2), # distance from text to points (x,y)\n",
        "                 ha='center') # horizontal alignment can be left, right or center\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[123705, 717667]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbZ9uo4erqso",
        "colab_type": "text"
      },
      "source": [
        "# Converting the dataset data frame into a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ezv09cQT6-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "edgeL=ss_df.to_numpy()\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vZLS5t5r11s",
        "colab_type": "text"
      },
      "source": [
        "# Step 1: Learning SLF Vectors for each node pair in the given graph\n",
        "\n",
        "### We learn the node embeddings in lower dimensions. This is also called Latent space embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmsRqArlt5_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1c819020-4af1-4bc8-81ce-070f53ff5bd8"
      },
      "source": [
        "W_out_p = np.matrix(np.zeros((max_node_id + 1, dim)), dtype=np.float32)\n",
        "W_in_p = np.matrix(np.zeros((max_node_id + 1, dim)), dtype=np.float32)\n",
        "W_out_n = np.matrix(np.zeros((max_node_id + 1, dim)), dtype=np.float32)\n",
        "W_in_n = np.matrix(np.zeros((max_node_id + 1, dim)), dtype=np.float32)\n",
        "\n",
        "for i in nodes:\n",
        "    for j in range(dim):\n",
        "        W_out_p[i, j] = rd.uniform(0, 1)\n",
        "        W_in_p[i, j] = rd.uniform(0, 1)\n",
        "        W_out_n[i, j] = rd.uniform(0, 1)\n",
        "        W_in_n[i, j] = rd.uniform(0, 1)\n",
        "\n",
        "start = time.time()\n",
        "for count in range(max_iter):\n",
        "    learning_rate = learning_rate_0 * (max_iter - count) / max_iter\n",
        "    for u in nodes:\n",
        "        out_p_g = np.zeros((1, dim))\n",
        "        out_n_g = np.zeros((1, dim))\n",
        "        in_p_g = np.zeros((1, dim))\n",
        "        in_n_g = np.zeros((1, dim))\n",
        "\n",
        "        succs = G.successors(u)\n",
        "        for succ in succs:\n",
        "            e_p = fa(W_out_p[u] @ W_in_p[succ].T)\n",
        "            e_n = fa(W_out_n[u] @ W_in_n[succ].T)\n",
        "            if G[u][succ]['weight'] == 1:\n",
        "                out_p_g += (1 - e_p) * W_in_p[succ]\n",
        "                out_n_g -= e_n * W_in_n[succ]\n",
        "            elif G[u][succ]['weight'] == -1:\n",
        "                out_p_g -= e_p * W_in_p[succ]\n",
        "                out_n_g += (1 - e_n) * W_in_n[succ]\n",
        "            elif G[u][succ]['weight'] == 0:\n",
        "                out_p_g += (1 - e_p) * W_in_p[succ]\n",
        "                out_n_g += (1 - e_n) * W_in_n[succ]\n",
        "        for i in range(noise_size):\n",
        "            noise = rd.choice(nodes)\n",
        "            while noise in succs:\n",
        "                noise = rd.choice(nodes)\n",
        "            e_p = fa(W_out_p[u] @ W_in_p[noise].T)\n",
        "            e_n = fa(W_out_n[u] @ W_in_n[noise].T)\n",
        "            out_p_g -= e_p * W_in_p[noise]\n",
        "            out_n_g -= e_n * W_in_n[noise]\n",
        "            \n",
        "        pres = G.predecessors(u)\n",
        "        for pre in pres:\n",
        "            e_p = fa(W_out_p[pre] @ W_in_p[u].T)\n",
        "            e_n = fa(W_out_n[pre] @ W_in_n[u].T)\n",
        "            if G[pre][u]['weight'] == 1:\n",
        "                in_p_g += (1 - e_p) * W_out_p[pre]\n",
        "                in_n_g -= e_n * W_out_n[pre]\n",
        "            elif G[pre][u]['weight'] == -1:\n",
        "                in_p_g -= e_p * W_out_p[pre]\n",
        "                in_n_g += (1 - e_n) * W_out_n[pre]\n",
        "            elif G[pre][u]['weight'] == 0:\n",
        "                in_p_g += (1 - e_p) * W_out_p[pre]\n",
        "                in_n_g += (1 - e_n) * W_out_n[pre]\n",
        "        for i in range(noise_size):\n",
        "            noise = rd.choice(nodes)\n",
        "            while noise in pres:\n",
        "                noise = rd.choice(nodes)\n",
        "            e_p = fa(W_out_p[noise] @ W_in_p[u].T)\n",
        "            e_n = fa(W_out_n[noise] @ W_in_n[u].T)\n",
        "            in_p_g -= e_p * W_out_p[noise]\n",
        "            in_n_g -= e_n * W_out_n[noise]\n",
        "\n",
        "        W_out_p[u] += learning_rate * out_p_g\n",
        "        W_in_p[u] += learning_rate * in_p_g\n",
        "        W_out_n[u] += learning_rate * out_n_g\n",
        "        W_in_n[u] += learning_rate * in_n_g\n",
        "\n",
        "        # Data format manipulation to avoid Pre-processing in next step. \n",
        "        # Handling Negative values and setting it to default 0\n",
        "        for i in range(dim):\n",
        "            if W_out_p[u, i] < 0:\n",
        "                W_out_p[u, i] = 0\n",
        "            if W_in_p[u, i] < 0:\n",
        "                W_in_p[u, i] = 0\n",
        "            if W_out_n[u, i] < 0:\n",
        "                W_out_n[u, i] = 0\n",
        "            if W_in_n[u, i] < 0:\n",
        "                W_in_n[u, i] = 0\n",
        "\n",
        "    end = time.time()\n",
        "    print('count' + str(count) + ' : ' + str(round(end - start, 1)) + 's')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count0 : 363.0s\n",
            "count1 : 718.5s\n",
            "count2 : 1069.5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpGSH2HsyhBA",
        "colab_type": "text"
      },
      "source": [
        "# Step 2: Concatenate SLF Vectors for all nodes. 'u'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtOoHb_by2AM",
        "colab_type": "text"
      },
      "source": [
        "### SLF Vectors are \n",
        "\n",
        "W_out_p(U out(u)): Positive outward SLF vectors\n",
        "\n",
        "W_in_p(U in(u)):   Positive inward SLF vectors\n",
        "\n",
        "W_out_n(W out(u)): Negative outward SLF vectors\n",
        "\n",
        "W_in_n(W in(u)):   Negative inward SLF vectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOoOF6Q5yfSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_out = np.matrix(np.zeros((max_node_id + 1, dim * 2)), dtype=np.float32)\n",
        "W_in = np.matrix(np.zeros((max_node_id + 1, dim * 2)), dtype=np.float32)\n",
        "for i in range(max_node_id + 1):\n",
        "    W_out[i, : dim] = W_out_p[i]\n",
        "    W_out[i, dim: dim * 2] = W_out_n[i]\n",
        "    W_in[i, : dim] = W_in_p[i]\n",
        "    W_in[i, dim: dim * 2] = W_in_n[i]\n",
        "\n",
        "# Uncomment this to save the SLF vectors (Node Embeddings)\n",
        "#np.savetxt(data_path + 'SLF_Out', W_out)\n",
        "#np.savetxt(data_path + 'SLF_in', W_in)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-nU69AKY83x",
        "colab_type": "text"
      },
      "source": [
        "### Sample SLF Vector with 64 dimensions for node 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwMcINwqaCxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0fbe7b46-2f8a-4fcb-c73f-8f6b842eb92b"
      },
      "source": [
        "print(W_out[0])\n",
        "type(W_out.shape[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.51599896 0.23066588 0.5256834  0.5333137  0.76475954 0.39888442\n",
            "  0.2898277  0.60454357 0.7629317  0.7814709  0.3332049  0.87338567\n",
            "  0.         0.15914473 0.43171936 0.13357255 0.11527102 0.46827832\n",
            "  0.85732484 0.46581113 0.19022383 0.8626902  0.1866262  0.7436437\n",
            "  0.22579925 0.42851812 0.12003399 0.10538498 0.4682952  0.7076523\n",
            "  0.11851142 0.81841063 0.56800455 0.         0.02236096 0.88942736\n",
            "  0.67006093 0.27050194 0.07498182 0.6604829  0.85936916 0.44851786\n",
            "  0.64065963 0.07791779 0.04765785 0.06406475 0.         0.4703441\n",
            "  0.05126408 0.7600286  0.8557629  0.4122692  0.6142124  0.06305169\n",
            "  0.7194402  0.41762418 0.42732993 0.8090263  0.66544795 0.12651557\n",
            "  0.3802965  0.66423726 0.09465966 0.11787736]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INpBBLmXZY5I",
        "colab_type": "text"
      },
      "source": [
        "### Train and test dataset split representation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZTUzjMDchZi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "44679cd2-3edf-434e-db43-adf7eb7c52dd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_edges, test_edges, = train_test_split(edgeL,test_size=0.3,random_state=16)\n",
        "\n",
        "for i, edge in enumerate(train_edges):\n",
        "  if(i==0):\n",
        "    print(edge)\n",
        "    print(i)\n",
        "    print(edge[0], edge[1],edge[2])\n",
        "    print(type(edge[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[11824 56146     1]\n",
            "0\n",
            "11824 56146 1\n",
            "<class 'numpy.int64'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bR_xWIuWUzn",
        "colab_type": "text"
      },
      "source": [
        "# Step 3: Combining(Concatenating) the node pair features for nodes u, v as a pair- pair(u,v) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnoW72O_fIUx",
        "colab_type": "text"
      },
      "source": [
        "In other words, it is nothing but Node feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO1bs3otWjxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "train_edges, test_edges, = train_test_split(edgeL,test_size=0.3,random_state=16)    \n",
        "    \n",
        "out_dim = int(W_out.shape[1])\n",
        "in_dim = int(W_in.shape[1])\n",
        "train_x = np.zeros((len(train_edges), (out_dim + in_dim) * 2))\n",
        "train_y = np.zeros((len(train_edges), 1))\n",
        "\n",
        "for i, edge in enumerate(train_edges):\n",
        "    u = int(edge[0])\n",
        "    v = int(edge[1])\n",
        "    train_x[i, : out_dim] = W_out[u]\n",
        "    train_x[i, out_dim: out_dim + in_dim] = W_in[u]\n",
        "    train_x[i, out_dim + in_dim: out_dim * 2 + in_dim] = W_out[v]\n",
        "    train_x[i, out_dim * 2 + in_dim:] = W_in[v]\n",
        "    if int(edge[2]) > 0:\n",
        "        train_y[i] = 1\n",
        "    else:\n",
        "        train_y[i] = -1\n",
        "\n",
        "test_x = np.zeros((len(test_edges), (out_dim + in_dim) * 2))\n",
        "test_y = np.zeros((len(test_edges), 1))\n",
        "for i, edge in enumerate(test_edges):\n",
        "    u = int(edge[0])\n",
        "    v = int(edge[1])\n",
        "    test_x[i, : out_dim] = W_out[u]\n",
        "    test_x[i, out_dim: out_dim + in_dim] = W_in[u]\n",
        "    test_x[i, out_dim + in_dim: out_dim * 2 + in_dim] = W_out[v]\n",
        "    test_x[i, out_dim * 2 + in_dim:] = W_in[v]\n",
        "    if int(edge[2]) > 0:\n",
        "        test_y[i] = 1\n",
        "    else:\n",
        "        test_y[i] = -1\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A14KdsdTfZUs",
        "colab_type": "text"
      },
      "source": [
        "# Step 4: Training the Model and Making Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFfGqdz_ffj3",
        "colab_type": "text"
      },
      "source": [
        "###1. Logistic Regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tusW8M0Mfh5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7e90086d-898e-47a4-e3dc-b93b8e4ed0e5"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "start = time.time()\n",
        "\n",
        "ss = StandardScaler()\n",
        "train_x = ss.fit_transform(train_x)\n",
        "test_x = ss.fit_transform(test_x)\n",
        "lr = LogisticRegressionCV(fit_intercept=True, max_iter=100, multi_class='multinomial', Cs=np.logspace(-2, 2, 20),\n",
        "                          cv=2, penalty=\"l2\", solver=\"lbfgs\", tol=0.01)\n",
        "lr.fit(train_x, train_y.ravel())\n",
        "pred_prob = lr.predict(test_x)\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print('Model training iteration time : ' + str(round(end - start, 1)) + 's') \n",
        "\n",
        "accuracy = accuracy_score(test_y.ravel(),pred_prob)\n",
        "\n",
        "print(\"Accuracy of Logistic regression model is \",str(round(accuracy*100,1)),\"%\")\n",
        "\n",
        "performance_val=roc_auc_score(test_y, pred_prob)\n",
        "print(\"The AUC Score of Logistic regression classification model is :\" ,str(round(performance_val,1)*100),\"%\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model training iteration time : 155.4s\n",
            "Accuracy of Logistic regression model is  91.8 %\n",
            "The AUC Score of Logistic regression classification model is : 80.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS--B5TX19oT",
        "colab_type": "text"
      },
      "source": [
        "### 2. LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoRSBetG2DU8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8534
        },
        "outputId": "cf107a56-3284-45f6-97e9-a623f6bc0a3c"
      },
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "train_data = lgb.Dataset(train_x, train_y.ravel())\n",
        "test_data = lgb.Dataset(test_x, test_y.ravel())\n",
        "\n",
        "# define parameters\n",
        "parameters = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'is_unbalance': 'true',\n",
        "    'feature_fraction': 0.5,\n",
        "    'bagging_fraction': 0.5,\n",
        "    'bagging_freq': 20,\n",
        "    'num_threads' : 2,\n",
        "    'seed' : 76\n",
        "}\n",
        "\n",
        "start=time.time()\n",
        "# train lightGBM model\n",
        "model = lgb.train(parameters,\n",
        "                   train_data,\n",
        "                   valid_sets=test_data,\n",
        "                   num_boost_round=1000,\n",
        "                   early_stopping_rounds=20)\n",
        "\n",
        "stop=time.time()\n",
        "\n",
        "lgbm_prob=model.predict(test_x)\n",
        "\n",
        "print(lgbm_prob[0:5])\n",
        "for i in range(0,252412):\n",
        "    if lgbm_prob[i]>=.5:       # setting threshold to .5\n",
        "       lgbm_prob[i]=1\n",
        "    else:  \n",
        "       lgbm_prob[i]=0\n",
        "\n",
        "print(lgbm_prob[0:5])\n",
        "accuracy_lgbm = accuracy_score(lgbm_prob,test_y.ravel())\n",
        "\n",
        "print(\"Accuracy of LightGBM model is \", str(round(accuracy_lgbm*100,1)),\"%\")\n",
        "\n",
        "performance_val=roc_auc_score(test_y, lgbm_prob)\n",
        "print(\"The AUC Score of Logistic regression classification model is :\" ,str(round(performance_val,1)*100),\"%\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\tvalid_0's auc: 0.872675\n",
            "Training until validation scores don't improve for 20 rounds.\n",
            "[2]\tvalid_0's auc: 0.89864\n",
            "[3]\tvalid_0's auc: 0.909317\n",
            "[4]\tvalid_0's auc: 0.915609\n",
            "[5]\tvalid_0's auc: 0.919026\n",
            "[6]\tvalid_0's auc: 0.922998\n",
            "[7]\tvalid_0's auc: 0.925795\n",
            "[8]\tvalid_0's auc: 0.927654\n",
            "[9]\tvalid_0's auc: 0.929102\n",
            "[10]\tvalid_0's auc: 0.930189\n",
            "[11]\tvalid_0's auc: 0.931406\n",
            "[12]\tvalid_0's auc: 0.932517\n",
            "[13]\tvalid_0's auc: 0.93328\n",
            "[14]\tvalid_0's auc: 0.933844\n",
            "[15]\tvalid_0's auc: 0.934582\n",
            "[16]\tvalid_0's auc: 0.935379\n",
            "[17]\tvalid_0's auc: 0.936303\n",
            "[18]\tvalid_0's auc: 0.937005\n",
            "[19]\tvalid_0's auc: 0.93778\n",
            "[20]\tvalid_0's auc: 0.938464\n",
            "[21]\tvalid_0's auc: 0.939185\n",
            "[22]\tvalid_0's auc: 0.939769\n",
            "[23]\tvalid_0's auc: 0.940486\n",
            "[24]\tvalid_0's auc: 0.941186\n",
            "[25]\tvalid_0's auc: 0.94172\n",
            "[26]\tvalid_0's auc: 0.942343\n",
            "[27]\tvalid_0's auc: 0.942948\n",
            "[28]\tvalid_0's auc: 0.943479\n",
            "[29]\tvalid_0's auc: 0.943955\n",
            "[30]\tvalid_0's auc: 0.944375\n",
            "[31]\tvalid_0's auc: 0.944885\n",
            "[32]\tvalid_0's auc: 0.945466\n",
            "[33]\tvalid_0's auc: 0.94595\n",
            "[34]\tvalid_0's auc: 0.946431\n",
            "[35]\tvalid_0's auc: 0.946673\n",
            "[36]\tvalid_0's auc: 0.946965\n",
            "[37]\tvalid_0's auc: 0.947276\n",
            "[38]\tvalid_0's auc: 0.947689\n",
            "[39]\tvalid_0's auc: 0.947941\n",
            "[40]\tvalid_0's auc: 0.94826\n",
            "[41]\tvalid_0's auc: 0.948749\n",
            "[42]\tvalid_0's auc: 0.949073\n",
            "[43]\tvalid_0's auc: 0.949492\n",
            "[44]\tvalid_0's auc: 0.949806\n",
            "[45]\tvalid_0's auc: 0.950065\n",
            "[46]\tvalid_0's auc: 0.950431\n",
            "[47]\tvalid_0's auc: 0.950682\n",
            "[48]\tvalid_0's auc: 0.950963\n",
            "[49]\tvalid_0's auc: 0.951371\n",
            "[50]\tvalid_0's auc: 0.95172\n",
            "[51]\tvalid_0's auc: 0.951958\n",
            "[52]\tvalid_0's auc: 0.952152\n",
            "[53]\tvalid_0's auc: 0.952482\n",
            "[54]\tvalid_0's auc: 0.952932\n",
            "[55]\tvalid_0's auc: 0.953149\n",
            "[56]\tvalid_0's auc: 0.953519\n",
            "[57]\tvalid_0's auc: 0.953748\n",
            "[58]\tvalid_0's auc: 0.954178\n",
            "[59]\tvalid_0's auc: 0.954532\n",
            "[60]\tvalid_0's auc: 0.954756\n",
            "[61]\tvalid_0's auc: 0.954871\n",
            "[62]\tvalid_0's auc: 0.95525\n",
            "[63]\tvalid_0's auc: 0.955612\n",
            "[64]\tvalid_0's auc: 0.955923\n",
            "[65]\tvalid_0's auc: 0.956064\n",
            "[66]\tvalid_0's auc: 0.956237\n",
            "[67]\tvalid_0's auc: 0.956431\n",
            "[68]\tvalid_0's auc: 0.956624\n",
            "[69]\tvalid_0's auc: 0.956825\n",
            "[70]\tvalid_0's auc: 0.956982\n",
            "[71]\tvalid_0's auc: 0.95711\n",
            "[72]\tvalid_0's auc: 0.957275\n",
            "[73]\tvalid_0's auc: 0.957426\n",
            "[74]\tvalid_0's auc: 0.957564\n",
            "[75]\tvalid_0's auc: 0.95771\n",
            "[76]\tvalid_0's auc: 0.957936\n",
            "[77]\tvalid_0's auc: 0.958147\n",
            "[78]\tvalid_0's auc: 0.958385\n",
            "[79]\tvalid_0's auc: 0.95856\n",
            "[80]\tvalid_0's auc: 0.958707\n",
            "[81]\tvalid_0's auc: 0.958874\n",
            "[82]\tvalid_0's auc: 0.959111\n",
            "[83]\tvalid_0's auc: 0.959174\n",
            "[84]\tvalid_0's auc: 0.959279\n",
            "[85]\tvalid_0's auc: 0.959465\n",
            "[86]\tvalid_0's auc: 0.959545\n",
            "[87]\tvalid_0's auc: 0.959657\n",
            "[88]\tvalid_0's auc: 0.95983\n",
            "[89]\tvalid_0's auc: 0.959957\n",
            "[90]\tvalid_0's auc: 0.960067\n",
            "[91]\tvalid_0's auc: 0.9602\n",
            "[92]\tvalid_0's auc: 0.960305\n",
            "[93]\tvalid_0's auc: 0.960368\n",
            "[94]\tvalid_0's auc: 0.960407\n",
            "[95]\tvalid_0's auc: 0.960523\n",
            "[96]\tvalid_0's auc: 0.960537\n",
            "[97]\tvalid_0's auc: 0.960657\n",
            "[98]\tvalid_0's auc: 0.96073\n",
            "[99]\tvalid_0's auc: 0.960851\n",
            "[100]\tvalid_0's auc: 0.960935\n",
            "[101]\tvalid_0's auc: 0.960996\n",
            "[102]\tvalid_0's auc: 0.961125\n",
            "[103]\tvalid_0's auc: 0.961187\n",
            "[104]\tvalid_0's auc: 0.961244\n",
            "[105]\tvalid_0's auc: 0.961343\n",
            "[106]\tvalid_0's auc: 0.961417\n",
            "[107]\tvalid_0's auc: 0.961497\n",
            "[108]\tvalid_0's auc: 0.961548\n",
            "[109]\tvalid_0's auc: 0.961656\n",
            "[110]\tvalid_0's auc: 0.961737\n",
            "[111]\tvalid_0's auc: 0.961817\n",
            "[112]\tvalid_0's auc: 0.961875\n",
            "[113]\tvalid_0's auc: 0.961972\n",
            "[114]\tvalid_0's auc: 0.962016\n",
            "[115]\tvalid_0's auc: 0.962046\n",
            "[116]\tvalid_0's auc: 0.962116\n",
            "[117]\tvalid_0's auc: 0.96216\n",
            "[118]\tvalid_0's auc: 0.962281\n",
            "[119]\tvalid_0's auc: 0.962357\n",
            "[120]\tvalid_0's auc: 0.962452\n",
            "[121]\tvalid_0's auc: 0.96252\n",
            "[122]\tvalid_0's auc: 0.962564\n",
            "[123]\tvalid_0's auc: 0.962611\n",
            "[124]\tvalid_0's auc: 0.962646\n",
            "[125]\tvalid_0's auc: 0.962701\n",
            "[126]\tvalid_0's auc: 0.962743\n",
            "[127]\tvalid_0's auc: 0.962771\n",
            "[128]\tvalid_0's auc: 0.962811\n",
            "[129]\tvalid_0's auc: 0.962852\n",
            "[130]\tvalid_0's auc: 0.962915\n",
            "[131]\tvalid_0's auc: 0.96296\n",
            "[132]\tvalid_0's auc: 0.963046\n",
            "[133]\tvalid_0's auc: 0.963099\n",
            "[134]\tvalid_0's auc: 0.96313\n",
            "[135]\tvalid_0's auc: 0.96319\n",
            "[136]\tvalid_0's auc: 0.963209\n",
            "[137]\tvalid_0's auc: 0.96327\n",
            "[138]\tvalid_0's auc: 0.963312\n",
            "[139]\tvalid_0's auc: 0.96338\n",
            "[140]\tvalid_0's auc: 0.96338\n",
            "[141]\tvalid_0's auc: 0.963432\n",
            "[142]\tvalid_0's auc: 0.963483\n",
            "[143]\tvalid_0's auc: 0.963525\n",
            "[144]\tvalid_0's auc: 0.963539\n",
            "[145]\tvalid_0's auc: 0.963614\n",
            "[146]\tvalid_0's auc: 0.963664\n",
            "[147]\tvalid_0's auc: 0.963714\n",
            "[148]\tvalid_0's auc: 0.963726\n",
            "[149]\tvalid_0's auc: 0.963726\n",
            "[150]\tvalid_0's auc: 0.963769\n",
            "[151]\tvalid_0's auc: 0.963833\n",
            "[152]\tvalid_0's auc: 0.963804\n",
            "[153]\tvalid_0's auc: 0.963842\n",
            "[154]\tvalid_0's auc: 0.963884\n",
            "[155]\tvalid_0's auc: 0.963901\n",
            "[156]\tvalid_0's auc: 0.963928\n",
            "[157]\tvalid_0's auc: 0.963953\n",
            "[158]\tvalid_0's auc: 0.963929\n",
            "[159]\tvalid_0's auc: 0.963958\n",
            "[160]\tvalid_0's auc: 0.964005\n",
            "[161]\tvalid_0's auc: 0.964036\n",
            "[162]\tvalid_0's auc: 0.964112\n",
            "[163]\tvalid_0's auc: 0.964135\n",
            "[164]\tvalid_0's auc: 0.964189\n",
            "[165]\tvalid_0's auc: 0.964214\n",
            "[166]\tvalid_0's auc: 0.964249\n",
            "[167]\tvalid_0's auc: 0.96427\n",
            "[168]\tvalid_0's auc: 0.964298\n",
            "[169]\tvalid_0's auc: 0.964324\n",
            "[170]\tvalid_0's auc: 0.964388\n",
            "[171]\tvalid_0's auc: 0.964415\n",
            "[172]\tvalid_0's auc: 0.964436\n",
            "[173]\tvalid_0's auc: 0.964349\n",
            "[174]\tvalid_0's auc: 0.964396\n",
            "[175]\tvalid_0's auc: 0.964416\n",
            "[176]\tvalid_0's auc: 0.964414\n",
            "[177]\tvalid_0's auc: 0.964452\n",
            "[178]\tvalid_0's auc: 0.964483\n",
            "[179]\tvalid_0's auc: 0.964508\n",
            "[180]\tvalid_0's auc: 0.964542\n",
            "[181]\tvalid_0's auc: 0.964588\n",
            "[182]\tvalid_0's auc: 0.964591\n",
            "[183]\tvalid_0's auc: 0.964588\n",
            "[184]\tvalid_0's auc: 0.964615\n",
            "[185]\tvalid_0's auc: 0.964643\n",
            "[186]\tvalid_0's auc: 0.964674\n",
            "[187]\tvalid_0's auc: 0.964728\n",
            "[188]\tvalid_0's auc: 0.964735\n",
            "[189]\tvalid_0's auc: 0.964738\n",
            "[190]\tvalid_0's auc: 0.964737\n",
            "[191]\tvalid_0's auc: 0.964746\n",
            "[192]\tvalid_0's auc: 0.964742\n",
            "[193]\tvalid_0's auc: 0.96481\n",
            "[194]\tvalid_0's auc: 0.964835\n",
            "[195]\tvalid_0's auc: 0.964815\n",
            "[196]\tvalid_0's auc: 0.964821\n",
            "[197]\tvalid_0's auc: 0.964819\n",
            "[198]\tvalid_0's auc: 0.96478\n",
            "[199]\tvalid_0's auc: 0.964828\n",
            "[200]\tvalid_0's auc: 0.964846\n",
            "[201]\tvalid_0's auc: 0.964855\n",
            "[202]\tvalid_0's auc: 0.96488\n",
            "[203]\tvalid_0's auc: 0.964908\n",
            "[204]\tvalid_0's auc: 0.964928\n",
            "[205]\tvalid_0's auc: 0.96496\n",
            "[206]\tvalid_0's auc: 0.964989\n",
            "[207]\tvalid_0's auc: 0.964989\n",
            "[208]\tvalid_0's auc: 0.964989\n",
            "[209]\tvalid_0's auc: 0.965014\n",
            "[210]\tvalid_0's auc: 0.965042\n",
            "[211]\tvalid_0's auc: 0.965057\n",
            "[212]\tvalid_0's auc: 0.965085\n",
            "[213]\tvalid_0's auc: 0.965117\n",
            "[214]\tvalid_0's auc: 0.965135\n",
            "[215]\tvalid_0's auc: 0.965145\n",
            "[216]\tvalid_0's auc: 0.965151\n",
            "[217]\tvalid_0's auc: 0.965165\n",
            "[218]\tvalid_0's auc: 0.965174\n",
            "[219]\tvalid_0's auc: 0.965174\n",
            "[220]\tvalid_0's auc: 0.965204\n",
            "[221]\tvalid_0's auc: 0.965248\n",
            "[222]\tvalid_0's auc: 0.965245\n",
            "[223]\tvalid_0's auc: 0.96525\n",
            "[224]\tvalid_0's auc: 0.965273\n",
            "[225]\tvalid_0's auc: 0.96529\n",
            "[226]\tvalid_0's auc: 0.96531\n",
            "[227]\tvalid_0's auc: 0.96532\n",
            "[228]\tvalid_0's auc: 0.965288\n",
            "[229]\tvalid_0's auc: 0.965307\n",
            "[230]\tvalid_0's auc: 0.965351\n",
            "[231]\tvalid_0's auc: 0.965367\n",
            "[232]\tvalid_0's auc: 0.965226\n",
            "[233]\tvalid_0's auc: 0.965209\n",
            "[234]\tvalid_0's auc: 0.965235\n",
            "[235]\tvalid_0's auc: 0.965273\n",
            "[236]\tvalid_0's auc: 0.965293\n",
            "[237]\tvalid_0's auc: 0.96533\n",
            "[238]\tvalid_0's auc: 0.965331\n",
            "[239]\tvalid_0's auc: 0.965355\n",
            "[240]\tvalid_0's auc: 0.965375\n",
            "[241]\tvalid_0's auc: 0.965315\n",
            "[242]\tvalid_0's auc: 0.96528\n",
            "[243]\tvalid_0's auc: 0.965303\n",
            "[244]\tvalid_0's auc: 0.965315\n",
            "[245]\tvalid_0's auc: 0.965327\n",
            "[246]\tvalid_0's auc: 0.965341\n",
            "[247]\tvalid_0's auc: 0.965316\n",
            "[248]\tvalid_0's auc: 0.965333\n",
            "[249]\tvalid_0's auc: 0.965282\n",
            "[250]\tvalid_0's auc: 0.965282\n",
            "[251]\tvalid_0's auc: 0.965279\n",
            "[252]\tvalid_0's auc: 0.965294\n",
            "[253]\tvalid_0's auc: 0.965344\n",
            "[254]\tvalid_0's auc: 0.965377\n",
            "[255]\tvalid_0's auc: 0.965401\n",
            "[256]\tvalid_0's auc: 0.965414\n",
            "[257]\tvalid_0's auc: 0.965434\n",
            "[258]\tvalid_0's auc: 0.965464\n",
            "[259]\tvalid_0's auc: 0.965492\n",
            "[260]\tvalid_0's auc: 0.965511\n",
            "[261]\tvalid_0's auc: 0.965521\n",
            "[262]\tvalid_0's auc: 0.965537\n",
            "[263]\tvalid_0's auc: 0.965558\n",
            "[264]\tvalid_0's auc: 0.96557\n",
            "[265]\tvalid_0's auc: 0.965621\n",
            "[266]\tvalid_0's auc: 0.965627\n",
            "[267]\tvalid_0's auc: 0.965638\n",
            "[268]\tvalid_0's auc: 0.965666\n",
            "[269]\tvalid_0's auc: 0.965676\n",
            "[270]\tvalid_0's auc: 0.965678\n",
            "[271]\tvalid_0's auc: 0.965695\n",
            "[272]\tvalid_0's auc: 0.965694\n",
            "[273]\tvalid_0's auc: 0.965716\n",
            "[274]\tvalid_0's auc: 0.965731\n",
            "[275]\tvalid_0's auc: 0.96574\n",
            "[276]\tvalid_0's auc: 0.965748\n",
            "[277]\tvalid_0's auc: 0.965768\n",
            "[278]\tvalid_0's auc: 0.965777\n",
            "[279]\tvalid_0's auc: 0.9658\n",
            "[280]\tvalid_0's auc: 0.965811\n",
            "[281]\tvalid_0's auc: 0.965853\n",
            "[282]\tvalid_0's auc: 0.965861\n",
            "[283]\tvalid_0's auc: 0.965883\n",
            "[284]\tvalid_0's auc: 0.965875\n",
            "[285]\tvalid_0's auc: 0.965882\n",
            "[286]\tvalid_0's auc: 0.965896\n",
            "[287]\tvalid_0's auc: 0.965914\n",
            "[288]\tvalid_0's auc: 0.96592\n",
            "[289]\tvalid_0's auc: 0.96594\n",
            "[290]\tvalid_0's auc: 0.96595\n",
            "[291]\tvalid_0's auc: 0.965978\n",
            "[292]\tvalid_0's auc: 0.965981\n",
            "[293]\tvalid_0's auc: 0.965995\n",
            "[294]\tvalid_0's auc: 0.966005\n",
            "[295]\tvalid_0's auc: 0.966018\n",
            "[296]\tvalid_0's auc: 0.966021\n",
            "[297]\tvalid_0's auc: 0.966034\n",
            "[298]\tvalid_0's auc: 0.966033\n",
            "[299]\tvalid_0's auc: 0.966054\n",
            "[300]\tvalid_0's auc: 0.966055\n",
            "[301]\tvalid_0's auc: 0.966089\n",
            "[302]\tvalid_0's auc: 0.966105\n",
            "[303]\tvalid_0's auc: 0.966108\n",
            "[304]\tvalid_0's auc: 0.966122\n",
            "[305]\tvalid_0's auc: 0.966135\n",
            "[306]\tvalid_0's auc: 0.966155\n",
            "[307]\tvalid_0's auc: 0.966192\n",
            "[308]\tvalid_0's auc: 0.966206\n",
            "[309]\tvalid_0's auc: 0.966213\n",
            "[310]\tvalid_0's auc: 0.966224\n",
            "[311]\tvalid_0's auc: 0.966226\n",
            "[312]\tvalid_0's auc: 0.966224\n",
            "[313]\tvalid_0's auc: 0.966232\n",
            "[314]\tvalid_0's auc: 0.966243\n",
            "[315]\tvalid_0's auc: 0.966266\n",
            "[316]\tvalid_0's auc: 0.966263\n",
            "[317]\tvalid_0's auc: 0.966289\n",
            "[318]\tvalid_0's auc: 0.966288\n",
            "[319]\tvalid_0's auc: 0.9663\n",
            "[320]\tvalid_0's auc: 0.966314\n",
            "[321]\tvalid_0's auc: 0.966321\n",
            "[322]\tvalid_0's auc: 0.966312\n",
            "[323]\tvalid_0's auc: 0.966315\n",
            "[324]\tvalid_0's auc: 0.966304\n",
            "[325]\tvalid_0's auc: 0.966336\n",
            "[326]\tvalid_0's auc: 0.966353\n",
            "[327]\tvalid_0's auc: 0.96636\n",
            "[328]\tvalid_0's auc: 0.966373\n",
            "[329]\tvalid_0's auc: 0.966387\n",
            "[330]\tvalid_0's auc: 0.966415\n",
            "[331]\tvalid_0's auc: 0.966427\n",
            "[332]\tvalid_0's auc: 0.966441\n",
            "[333]\tvalid_0's auc: 0.966451\n",
            "[334]\tvalid_0's auc: 0.966461\n",
            "[335]\tvalid_0's auc: 0.966464\n",
            "[336]\tvalid_0's auc: 0.966455\n",
            "[337]\tvalid_0's auc: 0.966464\n",
            "[338]\tvalid_0's auc: 0.966471\n",
            "[339]\tvalid_0's auc: 0.966484\n",
            "[340]\tvalid_0's auc: 0.966479\n",
            "[341]\tvalid_0's auc: 0.966512\n",
            "[342]\tvalid_0's auc: 0.966518\n",
            "[343]\tvalid_0's auc: 0.966526\n",
            "[344]\tvalid_0's auc: 0.966536\n",
            "[345]\tvalid_0's auc: 0.966539\n",
            "[346]\tvalid_0's auc: 0.966564\n",
            "[347]\tvalid_0's auc: 0.966574\n",
            "[348]\tvalid_0's auc: 0.966574\n",
            "[349]\tvalid_0's auc: 0.966571\n",
            "[350]\tvalid_0's auc: 0.966596\n",
            "[351]\tvalid_0's auc: 0.966604\n",
            "[352]\tvalid_0's auc: 0.966613\n",
            "[353]\tvalid_0's auc: 0.966632\n",
            "[354]\tvalid_0's auc: 0.966644\n",
            "[355]\tvalid_0's auc: 0.966662\n",
            "[356]\tvalid_0's auc: 0.96666\n",
            "[357]\tvalid_0's auc: 0.96666\n",
            "[358]\tvalid_0's auc: 0.96665\n",
            "[359]\tvalid_0's auc: 0.966652\n",
            "[360]\tvalid_0's auc: 0.966681\n",
            "[361]\tvalid_0's auc: 0.96671\n",
            "[362]\tvalid_0's auc: 0.966728\n",
            "[363]\tvalid_0's auc: 0.966731\n",
            "[364]\tvalid_0's auc: 0.966736\n",
            "[365]\tvalid_0's auc: 0.966756\n",
            "[366]\tvalid_0's auc: 0.966758\n",
            "[367]\tvalid_0's auc: 0.966778\n",
            "[368]\tvalid_0's auc: 0.966794\n",
            "[369]\tvalid_0's auc: 0.966811\n",
            "[370]\tvalid_0's auc: 0.966819\n",
            "[371]\tvalid_0's auc: 0.966825\n",
            "[372]\tvalid_0's auc: 0.96683\n",
            "[373]\tvalid_0's auc: 0.966828\n",
            "[374]\tvalid_0's auc: 0.966843\n",
            "[375]\tvalid_0's auc: 0.966856\n",
            "[376]\tvalid_0's auc: 0.966865\n",
            "[377]\tvalid_0's auc: 0.966872\n",
            "[378]\tvalid_0's auc: 0.966884\n",
            "[379]\tvalid_0's auc: 0.966878\n",
            "[380]\tvalid_0's auc: 0.966883\n",
            "[381]\tvalid_0's auc: 0.96688\n",
            "[382]\tvalid_0's auc: 0.966905\n",
            "[383]\tvalid_0's auc: 0.966912\n",
            "[384]\tvalid_0's auc: 0.966917\n",
            "[385]\tvalid_0's auc: 0.966918\n",
            "[386]\tvalid_0's auc: 0.966922\n",
            "[387]\tvalid_0's auc: 0.966936\n",
            "[388]\tvalid_0's auc: 0.966939\n",
            "[389]\tvalid_0's auc: 0.966946\n",
            "[390]\tvalid_0's auc: 0.966951\n",
            "[391]\tvalid_0's auc: 0.966958\n",
            "[392]\tvalid_0's auc: 0.966963\n",
            "[393]\tvalid_0's auc: 0.966969\n",
            "[394]\tvalid_0's auc: 0.966981\n",
            "[395]\tvalid_0's auc: 0.966982\n",
            "[396]\tvalid_0's auc: 0.966995\n",
            "[397]\tvalid_0's auc: 0.967002\n",
            "[398]\tvalid_0's auc: 0.967004\n",
            "[399]\tvalid_0's auc: 0.967012\n",
            "[400]\tvalid_0's auc: 0.96702\n",
            "[401]\tvalid_0's auc: 0.967014\n",
            "[402]\tvalid_0's auc: 0.967023\n",
            "[403]\tvalid_0's auc: 0.967022\n",
            "[404]\tvalid_0's auc: 0.967031\n",
            "[405]\tvalid_0's auc: 0.967035\n",
            "[406]\tvalid_0's auc: 0.967044\n",
            "[407]\tvalid_0's auc: 0.967062\n",
            "[408]\tvalid_0's auc: 0.967064\n",
            "[409]\tvalid_0's auc: 0.967067\n",
            "[410]\tvalid_0's auc: 0.967067\n",
            "[411]\tvalid_0's auc: 0.967074\n",
            "[412]\tvalid_0's auc: 0.96707\n",
            "[413]\tvalid_0's auc: 0.967065\n",
            "[414]\tvalid_0's auc: 0.967062\n",
            "[415]\tvalid_0's auc: 0.967079\n",
            "[416]\tvalid_0's auc: 0.967091\n",
            "[417]\tvalid_0's auc: 0.967093\n",
            "[418]\tvalid_0's auc: 0.967092\n",
            "[419]\tvalid_0's auc: 0.967097\n",
            "[420]\tvalid_0's auc: 0.967107\n",
            "[421]\tvalid_0's auc: 0.967107\n",
            "[422]\tvalid_0's auc: 0.967119\n",
            "[423]\tvalid_0's auc: 0.967112\n",
            "[424]\tvalid_0's auc: 0.967117\n",
            "[425]\tvalid_0's auc: 0.967118\n",
            "[426]\tvalid_0's auc: 0.967123\n",
            "[427]\tvalid_0's auc: 0.967143\n",
            "[428]\tvalid_0's auc: 0.967153\n",
            "[429]\tvalid_0's auc: 0.967157\n",
            "[430]\tvalid_0's auc: 0.967159\n",
            "[431]\tvalid_0's auc: 0.967164\n",
            "[432]\tvalid_0's auc: 0.96717\n",
            "[433]\tvalid_0's auc: 0.967171\n",
            "[434]\tvalid_0's auc: 0.967169\n",
            "[435]\tvalid_0's auc: 0.967181\n",
            "[436]\tvalid_0's auc: 0.967211\n",
            "[437]\tvalid_0's auc: 0.967215\n",
            "[438]\tvalid_0's auc: 0.967221\n",
            "[439]\tvalid_0's auc: 0.967228\n",
            "[440]\tvalid_0's auc: 0.967234\n",
            "[441]\tvalid_0's auc: 0.967253\n",
            "[442]\tvalid_0's auc: 0.967267\n",
            "[443]\tvalid_0's auc: 0.967289\n",
            "[444]\tvalid_0's auc: 0.96729\n",
            "[445]\tvalid_0's auc: 0.967299\n",
            "[446]\tvalid_0's auc: 0.967313\n",
            "[447]\tvalid_0's auc: 0.967323\n",
            "[448]\tvalid_0's auc: 0.967328\n",
            "[449]\tvalid_0's auc: 0.967326\n",
            "[450]\tvalid_0's auc: 0.967316\n",
            "[451]\tvalid_0's auc: 0.967335\n",
            "[452]\tvalid_0's auc: 0.967342\n",
            "[453]\tvalid_0's auc: 0.967349\n",
            "[454]\tvalid_0's auc: 0.967353\n",
            "[455]\tvalid_0's auc: 0.96736\n",
            "[456]\tvalid_0's auc: 0.967358\n",
            "[457]\tvalid_0's auc: 0.967361\n",
            "[458]\tvalid_0's auc: 0.967358\n",
            "[459]\tvalid_0's auc: 0.967371\n",
            "[460]\tvalid_0's auc: 0.967368\n",
            "[461]\tvalid_0's auc: 0.967378\n",
            "[462]\tvalid_0's auc: 0.967376\n",
            "[463]\tvalid_0's auc: 0.967384\n",
            "[464]\tvalid_0's auc: 0.967371\n",
            "[465]\tvalid_0's auc: 0.967366\n",
            "[466]\tvalid_0's auc: 0.967369\n",
            "[467]\tvalid_0's auc: 0.96738\n",
            "[468]\tvalid_0's auc: 0.96737\n",
            "[469]\tvalid_0's auc: 0.967376\n",
            "[470]\tvalid_0's auc: 0.967393\n",
            "[471]\tvalid_0's auc: 0.967405\n",
            "[472]\tvalid_0's auc: 0.967424\n",
            "[473]\tvalid_0's auc: 0.967417\n",
            "[474]\tvalid_0's auc: 0.967427\n",
            "[475]\tvalid_0's auc: 0.967279\n",
            "[476]\tvalid_0's auc: 0.967309\n",
            "[477]\tvalid_0's auc: 0.967336\n",
            "[478]\tvalid_0's auc: 0.967358\n",
            "[479]\tvalid_0's auc: 0.967364\n",
            "[480]\tvalid_0's auc: 0.967369\n",
            "[481]\tvalid_0's auc: 0.967375\n",
            "[482]\tvalid_0's auc: 0.967362\n",
            "[483]\tvalid_0's auc: 0.967368\n",
            "[484]\tvalid_0's auc: 0.967376\n",
            "[485]\tvalid_0's auc: 0.96738\n",
            "[486]\tvalid_0's auc: 0.967369\n",
            "[487]\tvalid_0's auc: 0.967369\n",
            "[488]\tvalid_0's auc: 0.96738\n",
            "[489]\tvalid_0's auc: 0.967378\n",
            "[490]\tvalid_0's auc: 0.967371\n",
            "[491]\tvalid_0's auc: 0.967376\n",
            "[492]\tvalid_0's auc: 0.967376\n",
            "[493]\tvalid_0's auc: 0.967377\n",
            "[494]\tvalid_0's auc: 0.967382\n",
            "Early stopping, best iteration is:\n",
            "[474]\tvalid_0's auc: 0.967427\n",
            "[0.91777804 0.93817626 0.89469961 0.99816505 0.98664289]\n",
            "[1. 1. 1. 1. 1.]\n",
            "Accuracy of LightGBM model is  77.8 %\n",
            "The AUC Score of Logistic regression classification model is : 90.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}